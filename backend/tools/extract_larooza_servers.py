"""
Larooza Server Extractor
Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø³ÙŠØ±ÙØ±Ø§Øª Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ù…Ù† Larooza Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„
"""
import asyncio
import sys
import os
import json
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote

class LaroozaServerExtractor:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ar,en-US;q=0.9,en;q=0.8",
            "Referer": "https://larooza.top/",
        }
        
    async def extract_servers(self, video_url: str):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ù…Ù† Ø±Ø§Ø¨Ø· Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        print(f"\n{'='*60}")
        print(f"ğŸ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ù…Ù†: {video_url}")
        print(f"{'='*60}\n")
        
        async with httpx.AsyncClient(timeout=30, verify=False, follow_redirects=True) as client:
            # 1. Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            print("ğŸ“¥ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ...")
            resp = await client.get(video_url, headers=self.headers)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
            title_tag = soup.find('h1')
            title = title_tag.get_text(strip=True) if title_tag else "Unknown"
            print(f"ğŸ“º Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {title}\n")
            
            # 2. Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© (play.php)
            play_url = video_url.replace('video.php', 'play.php')
            print(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø©: {play_url}")
            play_resp = await client.get(play_url, headers=self.headers)
            play_soup = BeautifulSoup(play_resp.text, 'html.parser')
            
            # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª
            servers = []
            
            # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† data-embed-url
            server_items = play_soup.select('li[data-embed-url]')
            print(f"\nâœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(server_items)} Ø³ÙŠØ±ÙØ± Ù…Ù† Ø®Ù„Ø§Ù„ data-embed-url\n")
            
            for idx, item in enumerate(server_items, 1):
                embed_url = item.get('data-embed-url')
                if not embed_url:
                    continue
                    
                if not embed_url.startswith('http'):
                    embed_url = urljoin(play_url, embed_url)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ø³ÙŠØ±ÙØ±
                name_tag = item.select_one('strong')
                name = name_tag.get_text(strip=True) if name_tag else f"Server {idx}"
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³Ù…
                name = name.replace("Ø³ÙŠØ±ÙØ±", "Server").strip()
                
                server_info = {
                    "id": idx,
                    "name": name,
                    "embed_url": embed_url,
                    "type": self._detect_server_type(embed_url)
                }
                
                servers.append(server_info)
                
                print(f"ğŸ¯ Ø§Ù„Ø³ÙŠØ±ÙØ± {idx}: {name}")
                print(f"   Ø§Ù„Ù†ÙˆØ¹: {server_info['type']}")
                print(f"   Ø§Ù„Ø±Ø§Ø¨Ø·: {embed_url}\n")
            
            # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† iframes (Ø§Ø­ØªÙŠØ§Ø·ÙŠ)
            if not servers:
                print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø³ÙŠØ±ÙØ±Ø§Øª Ù…Ù† data-embed-urlØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ iframes...\n")
                iframes = play_soup.find_all('iframe', src=True)
                
                for idx, iframe in enumerate(iframes, 1):
                    src = iframe['src']
                    if any(x in src.lower() for x in ['ads', 'google', 'facebook', 'analytics']):
                        continue
                        
                    if not src.startswith('http'):
                        src = urljoin(play_url, src)
                    
                    server_info = {
                        "id": idx,
                        "name": f"Server {idx}",
                        "embed_url": src,
                        "type": self._detect_server_type(src)
                    }
                    
                    servers.append(server_info)
                    print(f"ğŸ¯ Ø§Ù„Ø³ÙŠØ±ÙØ± {idx}: Server {idx}")
                    print(f"   Ø§Ù„Ù†ÙˆØ¹: {server_info['type']}")
                    print(f"   Ø§Ù„Ø±Ø§Ø¨Ø·: {src}\n")
            
            # 4. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„
            print(f"\n{'='*60}")
            print("ğŸ“¥ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„...")
            print(f"{'='*60}\n")
            
            download_links = await self._extract_downloads(client, video_url)
            
            # 5. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ù„Ù‚Ø§Øª (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø³Ù„Ø³Ù„)
            episodes = []
            if "Ø­Ù„Ù‚Ø©" in title or "Ø§Ù„Ø­Ù„Ù‚Ø©" in title:
                print(f"\n{'='*60}")
                print("ğŸ“º Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø­Ù„Ù‚Ø§Øª...")
                print(f"{'='*60}\n")
                episodes = self._extract_episodes(soup, video_url)
            
            # Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            result = {
                "title": title,
                "video_url": video_url,
                "play_url": play_url,
                "servers": servers,
                "download_links": download_links,
                "episodes": episodes,
                "total_servers": len(servers),
                "total_downloads": len(download_links),
                "total_episodes": len(episodes)
            }
            
            return result
    
    def _detect_server_type(self, url: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø³ÙŠØ±ÙØ± Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
        url_lower = url.lower()
        
        if 'vidbom' in url_lower or 'vidbem' in url_lower:
            return 'Vidbom'
        elif 'doodstream' in url_lower or 'dood' in url_lower:
            return 'Doodstream'
        elif 'voe.sx' in url_lower or 'voe' in url_lower:
            return 'VOE'
        elif 'okru' in url_lower or 'ok.ru' in url_lower:
            return 'OK.ru'
        elif 'vidmoly' in url_lower:
            return 'Vidmoly'
        elif 'filemoon' in url_lower:
            return 'Filemoon'
        elif 'streamtape' in url_lower:
            return 'Streamtape'
        elif 'uqload' in url_lower:
            return 'Uqload'
        elif 'larooza' in url_lower or 'okprime' in url_lower:
            return 'Larooza/OkPrime'
        elif 'short.icu' in url_lower:
            return 'Short.icu'
        else:
            return 'Unknown'
    
    async def _extract_downloads(self, client, video_url: str):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„"""
        download_links = []
        
        # Ø¬Ø±Ø¨ ØµÙØ­Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„
        dl_url = video_url.replace('video.php', 'download.php')
        
        try:
            resp = await client.get(dl_url, headers=self.headers)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # Ø§Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„
            for link in soup.select('a[href*="http"]'):
                href = link.get('href')
                text = link.get_text(strip=True)
                
                # ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„
                is_download = any(q in text.lower() or q in href.lower() 
                                for q in ['download', 'ØªØ­Ù…ÙŠÙ„', '720', '1080', '480', 'mp4', 'mkv'])
                
                if is_download and 'larooza' not in href:
                    quality = text if text else "Unknown Quality"
                    download_links.append({
                        "quality": quality,
                        "url": href
                    })
                    print(f"ğŸ“¥ {quality}: {href}")
        
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„: {e}")
        
        if not download_links:
            print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· ØªØ­Ù…ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø±Ø©")
        
        return download_links
    
    def _extract_episodes(self, soup: BeautifulSoup, base_url: str):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø­Ù„Ù‚Ø§Øª"""
        episodes = []
        
        # Ø§Ø¨Ø­Ø« Ø¹Ù† Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ø­Ù„Ù‚Ø§Øª
        episode_dropdowns = soup.select('select.episodeoption')
        
        for dropdown in episode_dropdowns:
            options = dropdown.find_all('option')
            for opt in options:
                href = opt.get('value')
                if not href or 'select-ep' in href or '#' in href:
                    continue
                
                full_url = urljoin(base_url, href)
                ep_text = opt.get_text(strip=True)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©
                import re
                match = re.search(r'(\d+)', ep_text)
                ep_num = int(match.group(1)) if match else 0
                
                if ep_num > 0:
                    episodes.append({
                        "episode": ep_num,
                        "title": ep_text,
                        "url": full_url
                    })
                    print(f"ğŸ“º Ø§Ù„Ø­Ù„Ù‚Ø© {ep_num}: {full_url}")
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ù†Ø³Ø¯Ù„Ø©ØŒ Ø§Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        if not episodes:
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text(strip=True)
                
                if 'Ø­Ù„Ù‚Ø©' in text and 'video.php?vid=' in href:
                    import re
                    match = re.search(r'(\d+)', text)
                    ep_num = int(match.group(1)) if match else 0
                    
                    full_url = urljoin(base_url, href)
                    
                    if ep_num > 0:
                        episodes.append({
                            "episode": ep_num,
                            "title": text,
                            "url": full_url
                        })
                        print(f"ğŸ“º Ø§Ù„Ø­Ù„Ù‚Ø© {ep_num}: {full_url}")
        
        return sorted(episodes, key=lambda x: x['episode'])

async def main():
    # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
    video_url = "https://larooza.top/video.php?vid=Yg22o3HXS"
    
    extractor = LaroozaServerExtractor()
    result = await extractor.extract_servers(video_url)
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù„Ø®Øµ
    print(f"\n{'='*60}")
    print("ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    print(f"{'='*60}\n")
    print(f"ğŸ“º Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {result['title']}")
    print(f"ğŸ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª: {result['total_servers']}")
    print(f"ğŸ“¥ Ø¹Ø¯Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„: {result['total_downloads']}")
    print(f"ğŸ“º Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù„Ù‚Ø§Øª: {result['total_episodes']}")
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ù…Ù„Ù JSON
    output_file = Path(__file__).parent / "larooza_servers_output.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_file}")
    
    return result

if __name__ == "__main__":
    asyncio.run(main())
